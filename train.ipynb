{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8rSl1dbDFWMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "id": "kXcPlVWwIqgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFrK-zyYw8I2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "%run driving_data.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "7iDCzuUjcyeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "id": "36vYYOiBOjE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run model.ipynb"
      ],
      "metadata": {
        "id": "Rgl0lx7YS10_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.core.protobuf import saver_pb2"
      ],
      "metadata": {
        "id": "pSiDzT4kUi-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOGDIR = './save'\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "L2NormConst = 0.001\n",
        "\n",
        "train_vars = tf.trainable_variables()\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(tf.subtract(y_, y))) + tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# create a summary to monitor cost tensor\n",
        "tf.summary.scalar(\"loss\", loss)\n",
        "# merge all summaries into a single op\n",
        "merged_summary_op = tf.summary.merge_all()\n",
        "\n",
        "saver = tf.train.Saver(write_version = saver_pb2.SaverDef.V2)\n",
        "\n",
        "# op to write logs to Tensorboard\n",
        "logs_path = './logs'\n",
        "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())"
      ],
      "metadata": {
        "id": "J3gocFNuxU8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "# train over the dataset about 30 times\n",
        "for epoch in range(epochs):\n",
        "  for i in range(int(num_images/batch_size)):\n",
        "    xs, ys = LoadTrainBatch(batch_size)\n",
        "    train_step.run(feed_dict={x: xs, y_: ys, keep_prob: 0.8})\n",
        "    if i % 10 == 0:\n",
        "      xs_train, ys_train = LoadTrainBatch(batch_size)\n",
        "      train_loss = loss.eval(feed_dict={x:xs_train, y_: ys_train, keep_prob: 1.0})\n",
        "      print(\"Epoch: %d, Step: %d, Training Loss: %g\" % (epoch, epoch * batch_size + i, train_loss))\n",
        "      xs_val, ys_val = LoadValBatch(batch_size)\n",
        "      val_loss = loss.eval(feed_dict={x:xs_val, y_: ys_val, keep_prob: 1.0})\n",
        "      print(\"Epoch: %d, Step: %d, Validation Loss: %g\" % (epoch, epoch * batch_size + i, val_loss))\n",
        "\n",
        "    # write logs at every iteration\n",
        "    summary = merged_summary_op.eval(feed_dict={x:xs, y_: ys, keep_prob: 1.0})\n",
        "    summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
        "\n",
        "    if i % batch_size == 0:\n",
        "      if not os.path.exists(LOGDIR):\n",
        "        os.makedirs(LOGDIR)\n",
        "      checkpoint_path = os.path.join(LOGDIR, \"model.ckpt\")\n",
        "      filename = saver.save(sess, checkpoint_path)\n",
        "  print(\"Model saved in file: %s\" % filename)\n",
        "\n",
        "# calculate and print testing loss\n",
        "test_loss = 0.0\n",
        "num_test_batches = int(num_test_images/batch_size)\n",
        "\n",
        "for i in range(num_test_batches):\n",
        "  xs_test, ys_test = LoadTestBatch(batch_size)\n",
        "  loss_value = loss.eval(feed_dict={x:xs_test, y_: ys_test, keep_prob: 1.0})\n",
        "  test_loss += loss_value\n",
        "\n",
        "test_loss /= num_test_batches\n",
        "\n",
        "print(\"Testing Loss: %g\" % test_loss)"
      ],
      "metadata": {
        "id": "3lLoRnBWMWrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "# train over the dataset about 30 times\n",
        "for epoch in range(epochs):\n",
        "  for i in range(int(num_images/batch_size)):\n",
        "    xs, ys = LoadTrainBatch(batch_size)\n",
        "    train_step.run(feed_dict={x: xs, y_: ys, keep_prob: 0.8})\n",
        "    if i % 10 == 0:\n",
        "      xs_train, ys_train = LoadTrainBatch(batch_size)\n",
        "      train_loss = loss.eval(feed_dict={x:xs_train, y_: ys_train, keep_prob: 1.0})\n",
        "      print(\"Epoch: %d, Step: %d, Training Loss: %g\" % (epoch, epoch * batch_size + i, train_loss))\n",
        "      xs_val, ys_val = LoadValBatch(batch_size)\n",
        "      val_loss = loss.eval(feed_dict={x:xs_val, y_: ys_val, keep_prob: 1.0})\n",
        "      print(\"Epoch: %d, Step: %d, Validation Loss: %g\" % (epoch, epoch * batch_size + i, val_loss))\n",
        "\n",
        "    # write logs at every iteration\n",
        "    summary = merged_summary_op.eval(feed_dict={x:xs, y_: ys, keep_prob: 1.0})\n",
        "    summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
        "\n",
        "    if i % batch_size == 0:\n",
        "      if not os.path.exists(LOGDIR):\n",
        "        os.makedirs(LOGDIR)\n",
        "      checkpoint_path = os.path.join(LOGDIR, \"model.ckpt\")\n",
        "      filename = saver.save(sess, checkpoint_path)\n",
        "  print(\"Model saved in file: %s\" % filename)\n",
        "\n",
        "# calculate and print testing loss\n",
        "test_loss = 0.0\n",
        "num_test_batches = int(num_test_images/batch_size)\n",
        "\n",
        "for i in range(num_test_batches):\n",
        "  xs_test, ys_test = LoadTestBatch(batch_size)\n",
        "  loss_value = loss.eval(feed_dict={x:xs_test, y_: ys_test, keep_prob: 1.0})\n",
        "  test_loss += loss_value\n",
        "\n",
        "test_loss /= num_test_batches\n",
        "\n",
        "print(\"Testing Loss: %g\" % test_loss)"
      ],
      "metadata": {
        "id": "-0fKiB9pMsX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "batch_size = 100\n",
        "\n",
        "# train over the dataset about 30 times\n",
        "for epoch in range(epochs):\n",
        "  for i in range(int(num_images/batch_size)):\n",
        "    xs, ys = LoadTrainBatch(batch_size)\n",
        "    train_step.run(feed_dict={x: xs, y_: ys, keep_prob: 0.8})\n",
        "    if i % 10 == 0:\n",
        "      xs_train, ys_train = LoadTrainBatch(batch_size)\n",
        "      train_loss = loss.eval(feed_dict={x:xs_train, y_: ys_train, keep_prob: 1.0})\n",
        "      print(\"Epoch: %d, Step: %d, Training Loss: %g\" % (epoch, epoch * batch_size + i, train_loss))\n",
        "      xs_val, ys_val = LoadValBatch(batch_size)\n",
        "      val_loss = loss.eval(feed_dict={x:xs_val, y_: ys_val, keep_prob: 1.0})\n",
        "      print(\"Epoch: %d, Step: %d, Validation Loss: %g\" % (epoch, epoch * batch_size + i, val_loss))\n",
        "\n",
        "    # write logs at every iteration\n",
        "    summary = merged_summary_op.eval(feed_dict={x:xs, y_: ys, keep_prob: 1.0})\n",
        "    summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
        "\n",
        "    if i % batch_size == 0:\n",
        "      if not os.path.exists(LOGDIR):\n",
        "        os.makedirs(LOGDIR)\n",
        "      checkpoint_path = os.path.join(LOGDIR, \"model.ckpt\")\n",
        "      filename = saver.save(sess, checkpoint_path)\n",
        "  print(\"Model saved in file: %s\" % filename)\n",
        "\n",
        "# calculate and print testing loss\n",
        "test_loss = 0.0\n",
        "num_test_batches = int(num_test_images/batch_size)\n",
        "\n",
        "for i in range(num_test_batches):\n",
        "  xs_test, ys_test = LoadTestBatch(batch_size)\n",
        "  loss_value = loss.eval(feed_dict={x:xs_test, y_: ys_test, keep_prob: 1.0})\n",
        "  test_loss += loss_value\n",
        "\n",
        "test_loss /= num_test_batches\n",
        "\n",
        "print(\"Testing Loss: %g\" % test_loss)"
      ],
      "metadata": {
        "id": "Uswm2z6GxIcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "5kNYxgsg5PDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "lBPfFw5j5Etd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}